{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Dataloading guide\n",
    "\n",
    "This section is intended to provide a brief introduction to the dataloading module and its main functionalities.\n",
    "\n",
    "In short, all functions and custom classes are designed to help you create an efficient Pytorch Dataloader to use during training. The main objective is to avoid loading the entire dataset all at once, but instead iteratively load (possibly overlapping) time windows called \"partitions\". A typical pipeline is based on the following steps:\n",
    "\n",
    "\n",
    "1) Define the **partition specs**, i.e. the EEGs' sampling rate, the window length and the overlap between consecutive windows.\n",
    "2) Call the **GetEEGPartitionNumber** function to extract the dataset length, i.e. the number of partitions which can be extracted from the EEG datasets, given the defined partition specs.\n",
    "3) Call the **GetEEGSplitTable** or the **GetEEGSplitTableKfold** function to split the data in train, validation and test sets.\n",
    "4) Pass the results of the previous points to the custom Pytorch Dataset **EEGDataset**\n",
    "5) Optional: create a custom Pytorch Sampler **EEGSampler**\n",
    "6) Create a **Pytorch Dataloader** with the custom Dataset (and Sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "First, let's import the dataloading module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append('..')  # Needed when running this from the selfeeg/doc folder\n",
    "from selfeeg import dataloading as dl\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# set seeds for reproducibility\n",
    "seed = 12\n",
    "torch.manual_seed( seed )\n",
    "np.random.seed( seed )\n",
    "random.seed( seed )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "To provide a simple and excecutable tutorial, we will create a fake collection of EEG datasets (already aligned) which we will save in a folder \"Simulated EEG\".\n",
    "Just to be clear, we will generate randn arrays of random length and save them. This is just to avoid downloading large datasets.\n",
    "\n",
    "To keep the size of the folder low, each file will be:\n",
    "1) a 2 Channel EEG\n",
    "2) random length between 1024 and 4096 samples\n",
    "3) Stored with name `\"{dataset_id}_{subject_id}_{session_id}_{trial_id}.pickle\"`. This will be useful for the split part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder if that not exists\n",
    "if not(os.path.isdir('Simulated_EEG')):\n",
    "    os.mkdir('Simulated_EEG')\n",
    "\n",
    "N=1000\n",
    "for i in range(N):\n",
    "    x = np.random.randn(2,np.random.randint(1024,4097))\n",
    "    y = np.random.randint(1,5)\n",
    "    sample = {'data': x, 'label': y}\n",
    "    dataset_id = (int(i//200)+1)\n",
    "    subject_id = (int( (i - 200*int(i//200)))//5+1)\n",
    "    session_id = (i%5+1)\n",
    "    trial_id   = 1\n",
    "    file_name = f'Simulated_EEG/{dataset_id}_{subject_id}_{session_id}_{trial_id}.pickle'\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(sample, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Now we have a folder with simulated 1000 EEGs coming from:\n",
    "1) 5 datasets (ID from 1 to 5);\n",
    "2) 40 subjects per dataset (ID from 1 to 40)\n",
    "3) 5 session per subject (ID from 1 to 5)\n",
    "\n",
    "Each file is a pickle file with a dictionary having keys:\n",
    "1) `'data'`: the numpy 2D array\n",
    "2) `'label`': a fake label associated to the EEG file (from 1 to 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## The GetEEGPartitionNumber function\n",
    "\n",
    "This function is important to calculate the dataset length once defined the partition specs. Let's suppose data have a sampling rate of 128 Hz, and we want to extract 2 seconds samples with a 15% overlap. \n",
    "\n",
    "To complicate things, let's assume that we want to remove the last half second of record, for example because it often has bad recorded data.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>WARNING:</b> \n",
    "    \n",
    "remember that this function is not omniscent, so we need to give a way to load the data. By default the function will try the scipy's `loadmat` function with the syntax\n",
    "`EEG = loadmat(path_to_file, simplify_cells=True)['DATA_STRUCT']['data'] ` \n",
    "which is the output of the BIDSalign library provided by our team\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define partition spec\n",
    "eegpath  = 'Simulated_EEG'\n",
    "freq     = 128        # sampling frequency in [Hz]\n",
    "overlap  = 0.15       # overlap between partitions\n",
    "window   = 2          # window length in [seconds]\n",
    "\n",
    "# define a function to load and transform data\n",
    "# SOME NOTES: these function can be fused to an unique one. Also, if\n",
    "# there's need to pass some arguments it's possible to pass them with\n",
    "# the optional_load_fun_args and optional_transform_fun_args arguments\n",
    "def loadEEG(path, return_label=False):\n",
    "    with open(path, 'rb') as handle:\n",
    "        EEG = pickle.load(handle)\n",
    "    x = EEG['data']\n",
    "    y = EEG['label']\n",
    "    if return_label:\n",
    "        return x, y\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def transformEEG(EEG):\n",
    "    EEG = EEG[:,:-64]\n",
    "    return EEG\n",
    "\n",
    "# call the function\n",
    "EEGlen = dl.get_eeg_partition_number(\n",
    "    eegpath,\n",
    "    freq,\n",
    "    window,\n",
    "    overlap,\n",
    "    file_format='*.pickle',\n",
    "    load_function=loadEEG,\n",
    "    optional_load_fun_args=[False],\n",
    "    transform_function=transformEEG\n",
    ")\n",
    "EEGlen.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## The GetEEGSplitTable function\n",
    "\n",
    "Now that we have a table with the exact number of samples associated to each EEG file, let's split the data.\n",
    "\n",
    "Split can be performed with different level of granularity (e.g. dataset, subject, file level), and can be performed in different ways, i.e. by giving the ID to put in a set, or simply the ratio. Also, some data can be excluded and, if you have a label (or a way to extract it) associated to the file, it is possible to perform a stratified split, with the ratio between label preserved, up to a certain limit, in each set.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>TIP</b> \n",
    "    \n",
    "you can also create a table for cross validation splits with the `GetEEGSplitTableKfold` function. Its functionalities are similar to the previous function, and if you want to extract a specific partition, you can use the `ExtractSplit` function.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>WARNING</b> \n",
    "\n",
    "stratification assume that EEG files at the split granulosity level share the same label. For example, if you want to split files at the subject level, be sure that all EEGs from the same subject are associated with the same labels, otherwise the split will not be excecuted in the right way. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "For now, let's assume we want to do a **stratified split** at the **file level**, but we want to **exclude EEGs from subjects 13 and 23 of each dataset**. Split ratios are **80/10/10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for stratified split we need to create an array with the labels\n",
    "# associated to each eeg file\n",
    "Labels = np.zeros(EEGlen.shape[0], dtype=int)\n",
    "for i in range(EEGlen.shape[0]):\n",
    "    _ , Labels[i] = loadEEG(EEGlen.iloc[i]['full_path'], return_label=True)\n",
    "\n",
    "EEGsplit = dl.get_eeg_split_table(\n",
    "    EEGlen, \n",
    "    test_ratio = 0.1,\n",
    "    val_ratio = 0.1,\n",
    "    test_split_mode = 'file',\n",
    "    val_split_mode = 'file',\n",
    "    exclude_data_id = None, #{x:[13,23] for x in range(1,6)},\n",
    "    stratified = True,\n",
    "    labels = Labels,\n",
    "    perseverance = 5000,\n",
    "    split_tolerance = 0.005,\n",
    "    seed = seed\n",
    ")\n",
    "dl.check_split(EEGlen, EEGsplit, Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "<br>Here is another example of a **non stratified split** at the **subject level** (EEG from the same subject in the same split set), but we want to **exclude EEGs from subjects 13 and 23 of each dataset**. Split ratios are **80/10/10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "EEGsplit2 = dl.get_eeg_split_table(\n",
    "    EEGlen,\n",
    "    test_ratio = 0.1,\n",
    "    val_ratio = 0.1,\n",
    "    test_split_mode = 'subject',\n",
    "    val_split_mode = 'subject',\n",
    "    exclude_data_id = {x:[13,23] for x in range(1,6)},\n",
    "    dataset_id_extractor = lambda x: int(x.split('_')[0]),\n",
    "    subject_id_extractor = lambda x: int(x.split('_')[1]),\n",
    "    perseverance = 5000,\n",
    "    split_tolerance = 0.005,\n",
    "    seed = seed\n",
    ")\n",
    "\n",
    "dl.check_split(EEGlen, EEGsplit2)\n",
    "\n",
    "# Considering the structure of the created dataset, \n",
    "# it's easy to look if splits are really subject based\n",
    "for i in range(EEGsplit2.shape[0]//5):\n",
    "    if EEGsplit2.iloc[(5*i):(5*i+5)]['split_set'].sum() not in [-5,0,5,10]:\n",
    "        # since split set is equal to -1, 0, 1, 2\n",
    "        # we just check that the sum of split set is five times one of such values\n",
    "        print('wrong_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "EEGsplit2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## The EEGDataset class\n",
    "\n",
    "Now we have all the ingredients necessary to initialize the custom dataset. The EEGDataset class is highly customizable, so we illustrate two examples, one usually employed for the pretraining, which doesn't involve the extraction of labels from the EEG files, and the other usually employed for fine-tuning, which instead use the labels.\n",
    "\n",
    "To initialize correctly the class EEGdataset you need :\n",
    "1. the output of the `GetEEGPartitionNumber` function (used to calculate the length)\n",
    "2. the output of the `GetEEGSplitTable` function (used to extract data of a specific split set)\n",
    "3. the partition spec as a **list** (format: \\[freq, window, overlap\\])\n",
    "\n",
    "other optional important parameters are:\n",
    "1. the mode (train, validation, test), used to select data from a specific split set\n",
    "2. the boolean 'supervised', used to tell if indexing using `[]` (the `__getitem__` method) must extract a label associated to the sample\n",
    "3. the label_on_load argument, used to tell if indexing using `[]` (the `__getitem__` method) will get the label from the loading function or it must call a custom function\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>TIP 1</b> \n",
    "    \n",
    "the class EEGDataset also accept custom functions to load, transform and get label from the EEG files.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>TIP 2</b> \n",
    "    \n",
    "if the label must be extracted from a dictionary, also with different files having the label inside a different key, check the label_key argument to handle that. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "**CASE 1: Pretraining - no label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pretrain = dl.EEGDataset(\n",
    "    EEGlen,\n",
    "    EEGsplit, \n",
    "    [freq, window, overlap], # split parameters must be given as list\n",
    "    mode = 'train',          # default, select all samples in the train set\n",
    "    load_function = loadEEG, \n",
    "    transform_function = transformEEG\n",
    ")\n",
    "sample_1 = dataset_pretrain[0]  # Grab the first sample\n",
    "print(sample_1.shape) # Note: the sample is automatically converted in a Tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "<br>**CASE 2: FineTuning - with label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_finetune = dl.EEGDataset(\n",
    "    EEGlen,\n",
    "    EEGsplit, \n",
    "    [freq, window, overlap], # split parameters must be given as list\n",
    "    mode = 'train',          # the default, select all samples in the train set\n",
    "    supervised = True,       # !!!!IMPORTANT!!!!\n",
    "    load_function = loadEEG,\n",
    "    optional_load_fun_args= [True], # tells loadEEG to return a label\n",
    "    transform_function=transformEEG,\n",
    "    label_on_load=True,      # the default \n",
    "    )\n",
    "sample_2, label_2 = dataset_finetune[0]  # grab the first sample\n",
    "print(sample_2.shape, label_2) # now we also have a label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## The EEGSampler\n",
    "\n",
    "Although optional, you can also create a custom sampler. The sampler allows creating 2 different types of iterator, which differently balance the trade-off between batch heterogeneity and batch creation speed:\n",
    "\n",
    "1. **Linear**: just returns a linear iterator. It is useful when you want to minimize the number of EEG file loading operations. However batches will contain cosecutive partitions of the same file, which could affect the operations of some layers like BatchNorm layers. To initialize the sampler in this mode, simply use the command <br> <code> EEGSampler( EEGDataset, Mode=0)</code>\n",
    "2. **Shuffled**: it returns a customized iterator. The iterator is constructed in this way:\n",
    "    1) Samples are shuffled at the file level;\n",
    "    2) Samples of the same file are shuffled;\n",
    "    3) Samples are rearranged based on the desired batch size and number of works. This step is performed to exploit the parallelization properties of the pytorch dataloader and reduce the number of loading operations. To initialize the sampler in this mode, simply use the command <br> <code> EEGSampler( EEGDataset, BatchSize, Workers )</code> \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>TIP</b> \n",
    "    \n",
    "We suggest to use the linear iterator for validation and test purpose since it's faster and does not require any batch heterogeneity.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "Here is a schematic representation of how Shuffled iterator is constructed, with **batch size = 5** and **workers = 4**\n",
    "\n",
    "![scheme](../Images/sampler_example.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_linear = dl.EEGSampler(dataset_pretrain, Mode=0)\n",
    "sampler_custom = dl.EEGSampler(dataset_pretrain, 16, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Final Dataloader\n",
    "\n",
    "Now simply put all together and create your custom Dataloader. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>WARNING</b> \n",
    "    \n",
    "If you have created a custom sampler, remember to also pass the same batch size and number of workers\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Dataloader = DataLoader(\n",
    "    dataset = dataset_pretrain,\n",
    "    batch_size = 16,\n",
    "    sampler = sampler_custom,\n",
    "    num_workers = 0\n",
    ")\n",
    "\n",
    "for X in Final_Dataloader:\n",
    "    print(X.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
