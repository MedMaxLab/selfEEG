{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6056e6-86ea-4c92-9e95-a64b38d6f4c9",
   "metadata": {},
   "source": [
    "# Use case with the EEGMMI Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790085d-fc09-4c01-9816-8c7e0fbd5115",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>WARNING</b> \n",
    "    \n",
    "This notebook includes multiple trainings that can extend its total run time. We strongly suggest to run it with a device with CUDA installed.\n",
    "\n",
    "</div>\n",
    "\n",
    "**Notebook Structure:**\n",
    "\n",
    "\n",
    "* [Introduction](#Introduction)\n",
    "* [Self-Supervised Learning Pipeline](#Self-Supervised-Learning-Pipeline)\n",
    "* [Dataset description and problem design](#Dataset-Description-and-problem-design)\n",
    "* [Notebook start](#Package-Import)\n",
    "  * [Data Preparation](#Data-Preparation)\n",
    "  * [Pretraining Phase](#Pretraining-Phase)\n",
    "    * [Define Dataloaders](#Define-Pretraining-dataloaders)\n",
    "    * [Define Augmenter](#Define-the-data-augmenter)\n",
    "    * [Define Pretraining Model](#Define-pretraining-model-and-other-training-objects)\n",
    "    * [Run Pretraining](#Pretrain-the-model)\n",
    "  * [Fine-Tuning Phase](#Fine-tuning-Phase)\n",
    "    * [Define Dataloaders](#Define-fine-tuning-dataloaders)\n",
    "    * [Model Transfer](#Define-fine-tuning-model-and-other-training-objects)\n",
    "    * [Run Fine-Tuning](#Fine-tuning)\n",
    "    * [Model Evaluation](#Evaluate-fine-tuned-model)\n",
    "  * [Comparison with Full Supervised Strategy](#Comparison-with-full-supervised)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b700a40-f4c2-45fc-8fc5-bd1f7952c9ec",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook will provide an additional example on how to use the selfeeg library on a real-world dataset. \n",
    "To do that, we will use the EEGMMI Dataset, which can be downloaded [**here**](https://physionet.org/content/eegmmidb/1.0.0/). However, to facilitate things, we will work with an already preprocessed version of such dataset made available as a resource for the book\n",
    "\n",
    "```\n",
    "Deep Learning for EEG-based Brain-Computer Interface: Representations, \n",
    "Algorithms and Applications, by Dr. Xiang Zhang and Prof. Lina Yao\n",
    "```\n",
    "\n",
    "Preprocessed data can be downloaded [**here**](https://github.com/neergaard/eegbci-data/tree/master)\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>INFO</b> \n",
    "    \n",
    "Self-Supervised Learning is usually beneficial when there is a small amount of supervision but several datasets to aggregate. In this case, we will work with a single dataset to keep things simple.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7965f413-5152-45f2-b1e6-bcf1fd8ccb2c",
   "metadata": {},
   "source": [
    "## Self-Supervised Learning Pipeline\n",
    "\n",
    "The notebook [**Build a self-supervised learning pipeline**](https://selfeeg.readthedocs.io/en/latest/SSL_guide.html) already provides a description on how to build a Self-Supervised Learning Pipeline with selfEEG. To summarize, typical steps include:\n",
    "\n",
    "1. PRETRAINING:\n",
    "    1. define the pretraining dataloaders\n",
    "    2. define the data augmenter\n",
    "    3. define the pretraining model and optional training elements\n",
    "    4. pretrain the model\n",
    "2. FINE-TUNING\n",
    "    1. define the fine-tuning dataloaders\n",
    "    2. define the fine-tuning model and optional training elements\n",
    "    3. transfer the pretrained encoder\n",
    "    4. fine-tune the model\n",
    "3. FINAL EVALUATION\n",
    "\n",
    "In addition, you can consult the following [**Review Paper**](https://ieeexplore.ieee.org/abstract/document/10365170), which provides additional references and datasets used to build EEG-Based Self-Supervised Learning Pipelines as well as a clear description on critical aspects to consider when designing SSL applications on biomedical signals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b3e034-f1cd-49ca-a6c2-2851ef6ac85c",
   "metadata": {},
   "source": [
    "## Dataset Description and problem design\n",
    "\n",
    "**Dataset Description**\n",
    "\n",
    "\n",
    "As described in the official documentation, the EEGMMI dataset consists of over 1500 one- and two-minute EEG recordings, obtained from 109 volunteers. Subjects performed different motor/imagery tasks while **64-channel EEG** were recorded using the **BCI2000 system**, with a sampling rate of **160 Hz**. In particolar, the following tasks were performed, giving a total of **14 records** per subjects:\n",
    "\n",
    "1. Baseline, eyes open\n",
    "2. Baseline, eyes closed\n",
    "3. Task 1 (open and close left or right fist)\n",
    "4. Task 2 (imagine opening and closing left or right fist)\n",
    "5. Task 3 (open and close both fists or both feet)\n",
    "6. Task 4 (imagine opening and closing both fists or both feet)\n",
    "7. Task 1\n",
    "8. Task 2\n",
    "9. Task 3\n",
    "10. Task 4\n",
    "11. Task 1\n",
    "12. Task 2\n",
    "13. Task 3\n",
    "14. Task 4\n",
    "\n",
    "**Designed Task**\n",
    "\n",
    "The investigated problem is a binary task based on *Task 2 (imagine opening and closing left or right fist)*. Task 2 data will be used for the *fine-tuning phase*, while data from *other tasks* will be used for the *pretraining phase*. Training, validation, and test sets will be created via subject-based splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd50e924-e2af-4c0b-8d2b-b3a27a0b51a9",
   "metadata": {},
   "source": [
    "## Package Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2813d14e-e23b-4cec-8490-6e92ed1bc02a",
   "metadata": {},
   "source": [
    "First, let's import all the packages necessary to run this notebook.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>WARNING</b> \n",
    "    \n",
    "to run this notebook you will also need <b>matplotlib</b> and <b>scikit-learn</b>, which are not dependecies of the selfeeg library. Be sure to install them in your environment.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732513d-39eb-4ddd-a6de-51e9574e515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT BASE PACKAGES\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append('..')  # needed if you run this inside the selfeeg/doc folder\n",
    "\n",
    "import selfeeg\n",
    "import selfeeg.augmentation as aug\n",
    "import selfeeg.dataloading as dl\n",
    "\n",
    "# IMPORT CLASSICAL PACKAGES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "# Draw figures inline with this notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# IMPORT TORCH\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# set seeds for reproducibility\n",
    "seed = 1234\n",
    "random.seed( seed )\n",
    "np.random.seed( seed )\n",
    "torch.manual_seed( seed )\n",
    "plt.style.use('seaborn-v0_8-white')\n",
    "plt.rcParams['figure.figsize'] = (15.0, 6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f25ac-e564-45df-9fb2-76edd8665d11",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "To run this notebook, it is necessary to download the preprocessed dataset from the following [**GitHub repository**](https://github.com/neergaard/eegbci-data/tree/master). In short, it necessary to:\n",
    "\n",
    "1. clone the repository with ``git clone https://github.com/xiangzhang1015/Deep-Learning-for-BCI.git`` (or download its zip version)\n",
    "2. Unzip all the files in the repository's ``dataset`` folder\n",
    "3. Put all the unzipped files in a folder with the name ``Data``, placed in the current working directory, or, alternatively, just set the path below accordingly\n",
    "\n",
    "Steps are simple but, just in case, here is a set of commands to include in a cell to automatically download and unzip the data in a Data folder\n",
    "\n",
    "    !mkdir Data\n",
    "    !wget https://github.com/xiangzhang1015/Deep-Learning-for-BCI/archive/refs/heads/master.zip\n",
    "    !unzip master.zip \"Deep-Learning-for-BCI-master/dataset/*\"\n",
    "    !mv -v Deep-Learning-for-BCI-master/dataset/* Data\n",
    "    !rm master.zip\n",
    "\n",
    "Downloaded data are 109 2-dimensional numpy arrays where each file collects all the records of the same subject, already well-preprocessed. \n",
    "\n",
    "Since we are working with a single dataset that can be totally preloaded, there's no need to use the **dataloading** module, which is designed for a collection of datasets impossible to directly load on your device. Therefore, we will just preload all the samples and create a basic Dataset class to fed to the Dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735d985c-b4f6-4f7c-8e34-0900abac44a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Set Dataset path -------------\n",
    "# If not this, change it with the right path\n",
    "dataPath = 'Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43149659-5653-4bb7-b8a7-ac319cddb0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MMI_subject_data(path: str,\n",
    "                         subject: int, \n",
    "                         pretrain_label: list[int]=[0,1,2,3,6,7,8,9], \n",
    "                         finetune_label: list[int]=[4,5],\n",
    "                         window: float=4.1,\n",
    "                         overlap: float=0., \n",
    "                        ):\n",
    "    \"\"\"\n",
    "    ``get_MMI_subject_data`` will load and separate subject-specific \n",
    "    data from the Physionet EEG Motor Movement/Imagery (EEGMMI) Dataset\n",
    "    for Self-Supervised Learning (SSL) applications. Loaded data are those \n",
    "    already well-preprocessed and provided as a resource for the book: \n",
    "    \n",
    "        Deep Learning for EEG-based Brain-Computer Interface: Representations, \n",
    "        Algorithms and Applications, by Dr. Xiang Zhang and Prof. Lina Yao \n",
    "\n",
    "    Data, tutorials, and more info about the provided format can be found \n",
    "    in the official git repository available at:\n",
    "        \n",
    "        https://github.com/xiangzhang1015/Deep-Learning-for-BCI\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        Path to the data root folder. It will be concatenated with the subject's file name.\n",
    "    subject: int\n",
    "        An integer defining the subject ID. Must be in [1, 109].\n",
    "    pretrain_label: list[int]\n",
    "        A list of label to put in the pretraining dataset. Must be in [0,10].\n",
    "        Values are associated with the following labels are: 0: open eyes,\n",
    "        1: close eyes, 2: left hand, 3: right hand, 4: image left hand,\n",
    "        5: image right hand, 6: open fists, 7: open feet, 8: image fist,\n",
    "        9: image feet, 10: rest. Default = [0,1,2,3,6,7,8,9]\n",
    "    finetune_label: list[int]\n",
    "        Same as pretrain_label but for the fine-tuning dataset. Default = [4,5]\n",
    "    window: float\n",
    "        The length of the window to consider. Must be a float lower than 4.1.\n",
    "        Default = 4.1\n",
    "    overlap: float\n",
    "        The overlap between consecutive windows of the same trial. Must be in \n",
    "        [0,1). Default = 0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pretrain_data: numpy array\n",
    "        Numpy array of shape [sample, channel, time] with all the subject's\n",
    "        samples to use for the pretraining\n",
    "    pretrain_lab: numpy array\n",
    "        Numpy array with the pretrain_data samples label\n",
    "    finetune_data: numpy array\n",
    "        Numpy array of shape [sample, channel, time] with all the subject's\n",
    "        samples to use for the fine-tuning\n",
    "    finetune_lab: numpy array\n",
    "        Numpy array with the finetune_data samples label\n",
    "    \n",
    "    \"\"\"\n",
    "    # Just some checks to make this function more robust\n",
    "    if int(subject)<1 or int(subject)>109:\n",
    "        raise ValueError('wrong subject ID')\n",
    "    if not(isinstance(path,str)):\n",
    "        raise ValueError('path must be a string')\n",
    "    elif path[-1] != os.sep:\n",
    "        path = path + os.sep\n",
    "    if len(set(pretrain_label).intersection(set(finetune_label)))>0:\n",
    "        raise ValueError('pretraining and finetuning labels must be different')\n",
    "    if window<0 or window>4.1:\n",
    "        raise ValueError('window must be in [0,4.1]') \n",
    "    \n",
    "    # load data\n",
    "    data = np.load(f'{dataPath}{int(subject)}.npy')\n",
    "    # get time points where the label change to speed up extraction\n",
    "    label_change = np.ediff1d(data[:,64]).nonzero()[0]\n",
    "    pretrain_data = []\n",
    "    pretrain_lab = []\n",
    "    finetune_data = []\n",
    "    finetune_lab = []\n",
    "    Nwindow = int(160*window) # Sampling rate is 160\n",
    "    Noverlap = int(Nwindow*overlap) # Round everything\n",
    "    idx = 0\n",
    "\n",
    "    # basically this block will scroll the loaded array from start to end\n",
    "    # jumping by (Nwindow-Noverlap) sample at a time. Conditions are:\n",
    "    # 1. The idx has a label to exclude, jump to the next segment\n",
    "    # 2. The idx has a label to include but there are not enough time samples\n",
    "    #    to fill the entire window. Jump to the next segment.\n",
    "    # 3. The ids has a label to include and there are enough samples to fill\n",
    "    #    the entire window. Add sample and jump by (Nwindow-Noverlap) steps.\n",
    "    while ( (idx+Nwindow) < data.shape[0]):\n",
    "        if data[idx,64] in pretrain_label:\n",
    "            if  data[idx+Nwindow-1,64] == data[idx,64]:\n",
    "                pretrain_data.append(data[idx:idx+Nwindow,:64])\n",
    "                pretrain_lab.append(data[idx,64])\n",
    "                idx = idx+Nwindow-Noverlap\n",
    "            else:\n",
    "                idx = label_change[np.searchsorted(label_change,[idx],side='right')[0]]+1\n",
    "        elif data[idx,64] in finetune_label:\n",
    "            if  data[idx+Nwindow-1,64] == data[idx,64]:\n",
    "                finetune_data.append(data[idx:idx+Nwindow,:64])\n",
    "                finetune_lab.append(data[idx,64])\n",
    "                idx = idx+Nwindow-Noverlap\n",
    "            else:\n",
    "                idx = label_change[np.searchsorted(label_change,[idx],side='right')[0]]+1\n",
    "        else:\n",
    "            idx = label_change[np.searchsorted(label_change,[idx],side='right')[0]]+1\n",
    "\n",
    "    pretrain_data = np.stack(pretrain_data, 0).astype('float32')\n",
    "    pretrain_data = np.transpose(pretrain_data, (0,2,1))\n",
    "    pretrain_lab  = np.array(pretrain_lab).astype('float32')\n",
    "    \n",
    "    finetune_data = np.stack(finetune_data, 0).astype('float32')\n",
    "    finetune_data = np.transpose(finetune_data, (0,2,1))\n",
    "    finetune_lab  = np.array(finetune_lab).astype('float32')\n",
    "    \n",
    "    return pretrain_data, pretrain_lab, finetune_data, finetune_lab\n",
    "\n",
    "\n",
    "def gather_subjects_data(path: str, \n",
    "                         subject_ids: list, \n",
    "                         pretrain_label: list[int]=[0,1,2,3,6,7,8,9], \n",
    "                         finetune_label: list[int]=[4,5],\n",
    "                         window: float=4.1,\n",
    "                         overlap: float=0.):\n",
    "    \"\"\"\n",
    "    ``gather_subjects_data`` calls the ``get_MMI_subject_data``\n",
    "    for each subject listed in subject_ids, then concatenate all the arrays.\n",
    "    \"\"\"\n",
    "    N = len(subject_ids)\n",
    "    Xpre  = [None]*N \n",
    "    Ypre  = [None]*N  \n",
    "    Xfine = [None]*N \n",
    "    Yfine = [None]*N \n",
    "    for n, i in enumerate(subject_ids):\n",
    "        Xpre[n], Ypre[n], Xfine[n], Yfine[n] = get_MMI_subject_data(dataPath, \n",
    "                                                subject= i, pretrain_label=pretrain_label,\n",
    "                                                finetune_label=finetune_label,\n",
    "                                                window=window, overlap=overlap)\n",
    "    Xpre  = torch.from_numpy( np.concatenate(Xpre,0))\n",
    "    Ypre  = torch.from_numpy( np.concatenate(Ypre,0))\n",
    "    Xfine = torch.from_numpy( np.concatenate(Xfine,0))\n",
    "    Yfine = torch.from_numpy( np.concatenate(Yfine,0))\n",
    "    return Xpre, Ypre, Xfine, Yfine\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f4b59-a324-44cb-a3d4-cb53fa8b0192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# random select subjects to put in train, val, and test\n",
    "all_id = [i for i in range(1, 110)]\n",
    "random.shuffle(all_id)\n",
    "train_id = all_id[0:70]\n",
    "val_id = all_id[70:70+17]\n",
    "test_id = all_id[70+17:]\n",
    "\n",
    "# create train tensors\n",
    "Xpre_train, Ypre_train, Xfine_train, Yfine_train = gather_subjects_data( path= dataPath,\n",
    "                                                                  subject_ids= train_id)\n",
    "# create validation tensors\n",
    "Xpre_val, Ypre_val, Xfine_val, Yfine_val = gather_subjects_data( path= dataPath,\n",
    "                                                                subject_ids= val_id)\n",
    "\n",
    "# There's no need to have a test set during pretraining\n",
    "_, _, Xfine_test, Yfine_test = gather_subjects_data( path= dataPath,subject_ids= test_id )\n",
    "\n",
    "# directly rescale and encode fine_tuning data\n",
    "Xfine_train = selfeeg.utils.scale_range_soft_clip(Xfine_train, 80, 3.5, 'uV', True)\n",
    "Yfine_train = (Yfine_train > 4).to(dtype=torch.float32)\n",
    "Xfine_val = selfeeg.utils.scale_range_soft_clip(Xfine_val, 80, 3.5, 'uV', True)\n",
    "Yfine_val = (Yfine_val > 4).to(dtype=torch.float32)\n",
    "Xfine_test = selfeeg.utils.scale_range_soft_clip(Xfine_test, 80, 3.5, 'uV', True)\n",
    "Yfine_test = (Yfine_test > 4).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757400f9-2a70-4802-8896-08dd44addf15",
   "metadata": {},
   "source": [
    "## Pretraining Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a7253-65e1-4ace-afc7-9c2c8fcadc2c",
   "metadata": {},
   "source": [
    "### Define Pretraining dataloaders\n",
    "\n",
    "Since we have directly preloaded the entire dataset, there is no need to use the custom classes and function of the selfEEG's dataloading module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27746e75-87ad-4c3e-8a9f-6bc0b7310d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some parameters\n",
    "Chan = 64\n",
    "freq = 160\n",
    "window = 4.1\n",
    "batchsize = 64\n",
    "nb_classes=2\n",
    "Samples = int(freq*window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e029f5-72db-46df-b5f0-cd4bf77b66df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGMMIDataset(Dataset):\n",
    "    def __init__(self, X, Y=None):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        if self.Y is None:\n",
    "            return self.X[index]\n",
    "        else:\n",
    "            return self.X[index], self.Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862b8c0-b7cc-466c-ac51-5aa35daff630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training dataloader\n",
    "trainset = EEGMMIDataset(Xpre_train)\n",
    "trainloader = DataLoader(dataset = trainset, batch_size= batchsize)\n",
    "\n",
    "# define validation dataloader\n",
    "valset = EEGMMIDataset(Xpre_val)\n",
    "valloader = DataLoader(dataset = valset, batch_size= batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415f14b-4d59-4628-a2f1-d81e2d3d414f",
   "metadata": {},
   "source": [
    "### Define the data augmenter\n",
    "\n",
    "Now we need to define an augmenter. To keep things simple, we define an augmenter which combines:\n",
    "\n",
    "1. the addition of some noise or channel lost from ``add_band_noise`` or ``masking``.\n",
    "2. the ``warp`` or ``crop_and_resize`` or ``permute`` augmentation.\n",
    "3. a final rescale of the range [-80, 80] uV in [-1, 1] with soft clipping with horizontal asintote of 3.5.\n",
    "\n",
    "This is similar to the augmentation proposed in the augmentation module introductory book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add4f69-1b57-4e19-b72f-3e7323feac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE AUGMENTER\n",
    "# First block: noise addition\n",
    "AUG_band = aug.DynamicSingleAug(aug.add_band_noise, \n",
    "                                 discrete_arg={'bandwidth': [\"delta\", \"theta\", \"alpha\", \"beta\", (30,49) ], \n",
    "                                               'samplerate': freq,'noise_range': 1}\n",
    "                               )\n",
    "AUG_mask = aug.DynamicSingleAug(aug.masking, discrete_arg = {'mask_number': [1,2,3,4], 'masked_ratio': 0.25})\n",
    "Block1 = aug.RandomAug( AUG_band, AUG_mask, p=[0.7, 0.3])\n",
    "\n",
    "# second block: warp or crop and resize\n",
    "AUG_crop = aug.DynamicSingleAug(aug.crop_and_resize,\n",
    "                                discrete_arg={'batch_equal': True},\n",
    "                                range_arg ={'N_cut': [1, 4], 'segments': [12,15]},\n",
    "                                range_type ={'N_cut': True, 'segments': True})\n",
    "\n",
    "AUG_permute = aug.DynamicSingleAug( aug.permutation_signal,\n",
    "                                    discrete_arg={'segments': [15], 'batch_equal': True},\n",
    "                                    range_arg ={'seg_to_per': [2, 5]},\n",
    "                                    range_type={'seg_to_per': True}\n",
    "                                  )\n",
    "AUG_warp = aug.DynamicSingleAug( aug.warp_signal,\n",
    "                                    discrete_arg={'segments': [10], 'batch_equal': True},\n",
    "                                    range_arg ={'squeeze_strength': [0.75,0.9], 'stretch_strength': [1.1,1.5]},\n",
    "                                    range_type={'squeeze_strength': False, 'stretch_strength': False}\n",
    "                                  )\n",
    "Block2 = aug.RandomAug( AUG_crop, AUG_permute, AUG_warp, p=[0.3,0.3,0.4])\n",
    "\n",
    "# third block: rescale\n",
    "Block3 = lambda x: selfeeg.utils.scale_range_soft_clip(x, 80, 3.5, 'uV', True)\n",
    "\n",
    "# FINAL AUGMENTER: SEQUENCE OF THE THREE RANDOM LISTS\n",
    "Augmenter = aug.SequentialAug(Block1, Block2, Block3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c01e0-1701-4c02-b908-b523fba80620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a random data augmentation\n",
    "Sample = trainset.__getitem__(random.randint(0,len(trainset)))\n",
    "t = np.linspace(0, Sample.shape[1]-1, Sample.shape[1])/freq\n",
    "SampleAug = Augmenter(Sample)\n",
    "RandChan= random.randint(0,Chan-1)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('time [s]', fontsize=15)\n",
    "ax1.set_ylabel('[uV]', color=color, fontsize=15)\n",
    "ax1.plot(t,  Sample[RandChan,:], color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color, labelsize=15)\n",
    "ax1.tick_params(axis='x', labelsize=15)\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:orange'\n",
    "ax2.set_ylabel('[ ]', color=color, fontsize=15)  # we already handled the x-label with ax1\n",
    "ax2.plot(t, SampleAug[RandChan,:],color=color, linewidth=2.5)\n",
    "ax2.tick_params(axis='y', labelsize=15, labelcolor=color)\n",
    "plt.title('Same random channel from one sample: augmented version', fontsize=20)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93fd97f-e732-4a2e-82be-eaac4c0db58a",
   "metadata": {},
   "source": [
    "### Define pretraining model and other training objects\n",
    "\n",
    "Now we need to define the pretraining model. To do that, we need to:\n",
    "\n",
    "1. instantiate an nn.Module defining the encoder (backbone)\n",
    "2. instantiate the right SSL module, giving the encoder and the network head's spec.\n",
    "\n",
    "For now, let's use a simple EEGNet with default parameters, and SimCLR as the SSL algorithm.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>NOTE 1</b> \n",
    "    \n",
    "each model in the <b>models</b> module have an extra class with only the encoder with the name <b>modelnameEncoder</b> (e.g., EEGNetEncoder). This will make model creation much easier. \n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>NOTE 2</b> \n",
    "    \n",
    "each model in the <b>ssl</b> module can accept a list or a nn.Module to create the network head. In case of a list, the head will be a sequence of dense layer with input and output size equal to the values of the list. Batchnorm and activation are based on the original works.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>WARNING</b> \n",
    "    \n",
    "Remember to check if the encoder output size matches the head input size. All modules in the ssl class doesn't check that.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a6e7e-98dd-4c4c-8884-295c7b523103",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Encoder\n",
    "NNencoder= selfeeg.models.EEGNetEncoder(Chans=Chan)\n",
    "\n",
    "# SSL model\n",
    "head_size=[ 320, 64, 32]\n",
    "SelfMdl = selfeeg.ssl.SimCLR(encoder=NNencoder, projection_head=head_size).to(device=device)\n",
    "\n",
    "# loss (fit method has a default loss based on the SSL algorithm)\n",
    "loss=selfeeg.losses.SimCLR_loss\n",
    "loss_arg={'temperature': 0.15}\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(SelfMdl.parameters(), lr=1e-3)\n",
    "\n",
    "# lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e211933-5c74-4acc-8cf3-99730f6c8fcf",
   "metadata": {},
   "source": [
    "### Pretrain the model\n",
    "\n",
    "Each SSL algorithm has an already implemented fit method, similar to scikitlearn or Keras. Of course it's not complete as the fit of bigger framewoks, but it certainly save you lots of lines of code and help you monitorate the training. Since this is just a tutorial we will run only 50 epochs of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a4a30-365f-4713-a6db-5617edfaabdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_info = SelfMdl.fit(train_dataloader = trainloader, \n",
    "                        augmenter=Augmenter, \n",
    "                        epochs=50,\n",
    "                        optimizer=optimizer, \n",
    "                        loss_func= loss, \n",
    "                        loss_args= loss_arg,\n",
    "                        lr_scheduler= scheduler, \n",
    "                        validation_dataloader=valloader,\n",
    "                        verbose=False, \n",
    "                        device= device, \n",
    "                        return_loss_info=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08381a75-fbe4-4153-b482-1934eaa2d2bb",
   "metadata": {},
   "source": [
    "## Fine-tuning Phase\n",
    "\n",
    "Now that the encoder is pretrained, let's perform fine-tuning. To do that, we need to recreate the right dataloaders and models. After that, the **selfeeg** library provides a **fine-tuning** function that is similar to the fit method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e8dc32-4b9a-45cd-9615-77f2ff00c3a8",
   "metadata": {},
   "source": [
    "### Define fine-tuning dataloaders\n",
    "\n",
    "This phase is basically the same as the previous one. The only differences are:\n",
    "\n",
    "1. We are using the fine-tuning data\n",
    "2. We are creating a test set for evaluation\n",
    "3. We need to extract a label from each sample.\n",
    "\n",
    "The used classes and methods are the same already used from the dataloading module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576a2c11-b6e7-4898-91bb-c7a19d096129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training dataloader\n",
    "trainsetFT = EEGMMIDataset(Xfine_train, Yfine_train)\n",
    "trainloaderFT = DataLoader(dataset = trainsetFT, batch_size= batchsize)\n",
    "\n",
    "# define validation dataloader\n",
    "valsetFT = EEGMMIDataset(Xfine_val, Yfine_val)\n",
    "valloaderFT = DataLoader(dataset = valsetFT, batch_size= batchsize, shuffle=False)\n",
    "\n",
    "# define test dataloader\n",
    "testsetFT = EEGMMIDataset(Xfine_test, Yfine_test)\n",
    "testloaderFT = DataLoader(dataset = testsetFT, batch_size= batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abb4ed0-f149-4a62-9d1d-ba32c0895c9f",
   "metadata": {},
   "source": [
    "### Define fine-tuning model and other training objects\n",
    "\n",
    "Remember that in this phase you need to transfer the pretrained encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb89f11-7ade-4722-b3b5-78f24cc34665",
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalMdl = selfeeg.models.EEGNet(nb_classes = 2, Chans = Chan, Samples = Samples)\n",
    "\n",
    "# Transfer the pretrained backbone and \n",
    "# move the final model to the chosen device\n",
    "SelfMdl.train() \n",
    "SelfMdl.to(device='cpu') \n",
    "FinalMdl.encoder = SelfMdl.get_encoder()\n",
    "FinalMdl.train()\n",
    "FinalMdl.to(device=device)\n",
    "\n",
    "# Define Loss\n",
    "def loss_fineTuning(yhat, ytrue):\n",
    "    yhat = torch.squeeze(yhat)\n",
    "    return F.binary_cross_entropy_with_logits(yhat, ytrue)\n",
    "\n",
    "# Define EarlyStopper\n",
    "earlystopFT = selfeeg.ssl.EarlyStopping(patience=20, min_delta=1e-03, \n",
    "                                        record_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069787cc-1a7a-4698-b777-fdd5918c3c33",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n",
    "Fine-tuning can be easily performed with the ``fine-tuning`` method.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>NOTE 1</b> \n",
    "    \n",
    "it is better to first pretrain only the new head, then update all model's weights.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9718dab2-4b6a-45d7-aef8-909d15d988db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer for warm-up\n",
    "optimizerFT = torch.optim.Adam(FinalMdl.parameters(), lr=0.001)\n",
    "\n",
    "# Define Optimizer \n",
    "gamma = 0.995\n",
    "optimizerFT2 = torch.optim.Adam(FinalMdl.parameters(), lr=0.0009)\n",
    "schedulerFT2 = torch.optim.lr_scheduler.ExponentialLR(optimizerFT2, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd036c7a-3de5-4898-a8b7-71869429f488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# START FINE-TUNING\n",
    "# First warm up by freezing the backbone\n",
    "do_warmup=True\n",
    "if do_warmup:\n",
    "    FinalMdl.encoder.requires_grad_(False)\n",
    "    finetuning_loss=selfeeg.ssl.fine_tune(model         = FinalMdl,\n",
    "                                  train_dataloader      = trainloaderFT,\n",
    "                                  epochs                = 20,\n",
    "                                  optimizer             = optimizerFT,\n",
    "                                  loss_func             = loss_fineTuning, \n",
    "                                  validation_dataloader = valloaderFT,\n",
    "                                  verbose               = False,\n",
    "                                  device                = device,\n",
    "                                  return_loss_info      = True\n",
    "                                  )\n",
    "if do_warmup:\n",
    "    FinalMdl.encoder.requires_grad_(True)\n",
    "\n",
    "finetuning_loss=selfeeg.ssl.fine_tune(model         = FinalMdl,\n",
    "                              train_dataloader      = trainloaderFT,\n",
    "                              epochs                = 150,\n",
    "                              optimizer             = optimizerFT2,\n",
    "                              loss_func             = loss_fineTuning, \n",
    "                              lr_scheduler          = schedulerFT2,\n",
    "                              EarlyStopper          = earlystopFT,\n",
    "                              validation_dataloader = valloaderFT,\n",
    "                              verbose               = False,\n",
    "                              device                = device,\n",
    "                              return_loss_info      = True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204489d-2c80-41c0-a488-2e4cbc80ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(finetuning_loss))],\n",
    "         [finetuning_loss[i][0] for i in range(len(finetuning_loss))],\n",
    "         linewidth=2.5)\n",
    "plt.plot([i for i in range(len(finetuning_loss))],\n",
    "         [finetuning_loss[i][1] for i in range(len(finetuning_loss))],\n",
    "         linewidth=2.5)\n",
    "plt.title('Loss Progression Full Supervised')\n",
    "plt.xlabel('Epoch', fontsize=15)\n",
    "plt.ylabel('Loss', fontsize=15)\n",
    "plt.legend(['training', 'validation'],fontsize=15 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c66d5a-e56b-4bc3-9bee-0943880ed454",
   "metadata": {},
   "source": [
    "## Evaluate fine-tuned model\n",
    "\n",
    "Now you can evaluate your model in whatever method you prefer. Here is a simple example with the classification report from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b9201-f10d-460d-8722-50984f66b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes=2\n",
    "FinalMdl.to(device=device)\n",
    "FinalMdl.eval()\n",
    "ytrue=torch.zeros(len(testloaderFT.dataset))\n",
    "ypred=torch.zeros_like(ytrue)\n",
    "cnt=0\n",
    "for i, (X, Y) in enumerate(testloaderFT):\n",
    "    X=X.to(device=device)\n",
    "    ytrue[cnt:cnt+X.shape[0]]= Y \n",
    "    with torch.no_grad():\n",
    "        yhat = torch.sigmoid(FinalMdl(X)).to(device='cpu')\n",
    "        ypred[cnt:cnt+X.shape[0]] = torch.squeeze(yhat) \n",
    "    cnt += X.shape[0]\n",
    "\n",
    "print('Results of trivial Example\\n')\n",
    "print(classification_report(ytrue,ypred>0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70248ee-5f8e-4821-8038-fe761bd6f2f9",
   "metadata": {},
   "source": [
    "## Comparison with full supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38026248-261c-4bb4-a6c4-076865669b7e",
   "metadata": {},
   "source": [
    "Just for comparison, let's see if there is at least a slightly improvement compared to a standard full supervised pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686d313-e48d-4ca0-866b-8d37a2816f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Additional Model is EEGNet, which will perform slightly better in this case\n",
    "FinalMdl2 = selfeeg.models.EEGNet(nb_classes = 2, Chans = Chan, Samples = Samples)\n",
    "\n",
    "FinalMdl2.to(device=device)\n",
    "\n",
    "gamma = 0.995\n",
    "optimizerFT = torch.optim.Adam(FinalMdl2.parameters(), lr=0.0001)\n",
    "schedulerFT = torch.optim.lr_scheduler.ExponentialLR(optimizerFT, gamma=gamma)\n",
    "\n",
    "earlystopFT2 = selfeeg.ssl.EarlyStopping(patience=10, min_delta=1e-03, \n",
    "                                        record_best_weights=True)\n",
    "\n",
    "finetuning_loss=selfeeg.ssl.fine_tune(model         = FinalMdl2,\n",
    "                              train_dataloader      = trainloaderFT,\n",
    "                              epochs                = 150,\n",
    "                              optimizer             = optimizerFT,\n",
    "                              loss_func             = loss_fineTuning, \n",
    "                              lr_scheduler          = schedulerFT,\n",
    "                              EarlyStopper          = earlystopFT2,\n",
    "                              validation_dataloader = valloaderFT,\n",
    "                              verbose               = False,\n",
    "                              device                = device,\n",
    "                              return_loss_info      = True\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed95be5-e19d-43b3-a61e-a21dc320fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(finetuning_loss))],\n",
    "         [finetuning_loss[i][0] for i in range(len(finetuning_loss))],\n",
    "         linewidth=2.5)\n",
    "plt.plot([i for i in range(len(finetuning_loss))],\n",
    "         [finetuning_loss[i][1] for i in range(len(finetuning_loss))],\n",
    "         linewidth=2.5)\n",
    "plt.title('Loss Progression Full Supervised')\n",
    "plt.xlabel('Epoch', fontsize=15)\n",
    "plt.ylabel('Loss', fontsize=15)\n",
    "plt.legend(['training', 'validation'],fontsize=15 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9021a27-d749-4e0f-b1a6-e1ef4f00832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalMdl2.to(device=device)\n",
    "FinalMdl2.eval()\n",
    "ytrue=torch.zeros(len(testloaderFT.dataset))\n",
    "ypred=torch.zeros_like(ytrue)\n",
    "cnt=0\n",
    "for i, (X, Y) in enumerate(testloaderFT):\n",
    "    X=X.to(device=device)\n",
    "    ytrue[cnt:cnt+X.shape[0]]= Y \n",
    "    with torch.no_grad():\n",
    "        yhat = torch.sigmoid(FinalMdl2(X)).to(device='cpu')\n",
    "        ypred[cnt:cnt+X.shape[0]] = torch.squeeze(yhat) \n",
    "    cnt += X.shape[0]\n",
    "\n",
    "print('Results of trivial Example\\n')\n",
    "print(classification_report(ytrue,ypred>0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027bb03b-00cf-4ea1-9a40-6ac46393eb1a",
   "metadata": {},
   "source": [
    "We can see that the pretrained model has achieved a better validation loss in less epochs compared to the full-supervised strategy. Accuracies and F1-scores are almost identical, but this is expected considering that this tutorial uses only a small dataset and EEGNet, which is a very small model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
