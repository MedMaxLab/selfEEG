{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(os.getcwd().split('/Notebooks/test_notebook')[0])\n",
    "import itertools\n",
    "import platform\n",
    "\n",
    "# IMPORT CLASSICAL PACKAGES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# IMPORT TORCH\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# IMPORT CUSTOM SELF-SUPERVISED LEARNING FOR EEG LIBRARY\n",
    "from selfeeg import augmentation as aug\n",
    "from selfeeg import dataloading as dl\n",
    "from selfeeg import models as zoo\n",
    "from selfeeg import ssl\n",
    "from selfeeg import losses\n",
    "from selfeeg import utils\n",
    "\n",
    "def create_dataset(folder_name='Simulated_EEG',\n",
    "                   Sample_range= [512, 1025],\n",
    "                   Chans = 16,\n",
    "                   return_labels = True,\n",
    "                   seed=1234):\n",
    "    N=1000\n",
    "    if not(os.path.isdir(folder_name)):\n",
    "        os.mkdir(folder_name)\n",
    "\n",
    "    np.random.seed(seed=seed)\n",
    "    classes = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        Sample = np.random.randint(Sample_range[0],Sample_range[1])\n",
    "        y = np.random.choice([0,1], p=[0.8,0.2])\n",
    "        classes[i] = y\n",
    "        x = 600\n",
    "        while (np.max(x)>550 or np.min(x)<-550):\n",
    "            if y == 1:\n",
    "                stderr = np.sqrt(122.35423)\n",
    "                F1 = np.random.normal(0.932649, 0.040448)\n",
    "                F0 = np.random.normal(2.1159355, 2.3523977)\n",
    "            else:\n",
    "                stderr = np.sqrt(454.232666)\n",
    "                F1 = np.random.normal(0.9619603, 0.0301687)\n",
    "                F0 = np.random.normal(-0.1810323, 3.4712047)\n",
    "            x = np.zeros((Chans,Sample))\n",
    "            x[:,0] = np.random.normal( 0, stderr, Chans )  \n",
    "            for k in range(1,Sample):\n",
    "                x[:,k] = F0+ F1*x[:,k-1] + np.random.normal( 0, stderr, Chans )\n",
    "                \n",
    "        sample = {'data': x, 'label': y}\n",
    "        A, B, C = (int(i//200)+1), (int( (i - 200*int(i//200)))//5+1), (i%5+1)\n",
    "        file_name = 'Simulated_EEG/' + str(A) + '_' + str(B) + '_' + str(C) + '_1.pickle'\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(sample, f)\n",
    "    if return_labels:\n",
    "        return classes\n",
    "\n",
    "def loadEEG(path, return_label=False):\n",
    "    with open(path, 'rb') as handle:\n",
    "        EEG = pickle.load(handle)\n",
    "    x = EEG['data']\n",
    "    y = EEG['label']\n",
    "    if return_label:\n",
    "        return x, y\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def transformEEG(EEG, value=64):\n",
    "    EEG = EEG[:,:-value]\n",
    "    return EEG\n",
    "\n",
    "def makeGrid(pars_dict):  \n",
    "    keys=pars_dict.keys()\n",
    "    combinations=itertools.product(*pars_dict.values())\n",
    "    ds=[dict(zip(keys,cc)) for cc in combinations]\n",
    "    return ds\n",
    "\n",
    "\n",
    "device  = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "if device.type == 'cpu':\n",
    "    device  = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------------------')\n",
    "print('TESTING SSL MODULE')\n",
    "if device.type != 'cpu':\n",
    "    print('Found cuda device: testing ssl module on it')\n",
    "else:\n",
    "    print('Didn\\'t found cuda device: testing ssl module on cpu')\n",
    "print('---------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE FILE PATH, SAMPLING RATE, WINDOW LENGTH, OVERLAP PERCENTAGE, WORKERS AND BATCH SIZE\n",
    "eegpath = 'Simulated_EEG'\n",
    "freq = 128\n",
    "window = 2\n",
    "overlap = 0.1\n",
    "workers = 4\n",
    "batchsize = 16\n",
    "Chan = 16\n",
    "\n",
    "classes = create_dataset()\n",
    "\n",
    "# CALCULATE DATASET LENGTH\n",
    "EEGlen = dl.GetEEGPartitionNumber(eegpath, freq, window, overlap, file_format='*.pickle', \n",
    "                                  load_function=loadEEG)\n",
    "\n",
    "# SPLIT DATASET\n",
    "EEGsplit= dl.GetEEGSplitTable(partition_table=EEGlen, val_ratio= 0.1, stratified=True, labels=classes,\n",
    "                              test_data_id=[5], split_tolerance=0.001, perseverance=5000)\n",
    "\n",
    "# DEFINE TRAINING DATALOADER\n",
    "trainset = dl.EEGDataset(EEGlen, EEGsplit, [freq, window, overlap], load_function=loadEEG)\n",
    "trainsampler = dl.EEGsampler(trainset, batchsize, workers)\n",
    "trainloader = DataLoader(dataset = trainset, batch_size= batchsize, sampler=trainsampler, num_workers=workers)\n",
    "\n",
    "# DEFINE VALIDATION DATALOADER\n",
    "valset = dl.EEGDataset(EEGlen, EEGsplit, [freq, window, overlap], 'validation', load_function=loadEEG)\n",
    "valloader = DataLoader(dataset = valset, batch_size= batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE AUGMENTER\n",
    "# First block: noise addition\n",
    "AUG_band = aug.DynamicSingleAug(aug.add_band_noise, \n",
    "                                 discrete_arg={'bandwidth': [\"delta\", \"theta\", \"alpha\", \"beta\", (30,49) ], \n",
    "                                               'samplerate': freq,'noise_range': 0.5}\n",
    "                               )\n",
    "AUG_mask = aug.DynamicSingleAug(aug.masking, discrete_arg = {'mask_number': [1,2,3,4], 'masked_ratio': 0.25})\n",
    "Block1 = aug.RandomAug( AUG_band, AUG_mask, p=[0.7, 0.3])\n",
    "\n",
    "# second block: rescale\n",
    "Block2 = lambda x: utils.scale_range_soft_clip(x, 500, 1.5, 'uV', True)\n",
    "\n",
    "# FINAL AUGMENTER: SEQUENCE OF THE THREE RANDOM LISTS\n",
    "Augmenter = aug.SequentialAug(Block1, Block2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSL model\n",
    "emb_size= 16*((freq*window)//int(4*8))\n",
    "head_size=[ emb_size, 128, 64]\n",
    "predictor_size= [64, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing SimCLR (5 epochs, verbose True)...')\n",
    "\n",
    "NNencoder= zoo.EEGNetEncoder(Chans=Chan, kernLength=65)\n",
    "SelfMdl = ssl.SimCLR(encoder=NNencoder, projection_head=head_size).to(device=device)\n",
    "\n",
    "# loss (fit method has a default loss based on the SSL algorithm\n",
    "loss=losses.SimCLR_loss\n",
    "loss_arg={'temperature': 0.5}\n",
    "\n",
    "# earlystopper\n",
    "earlystop = ssl.EarlyStopping(patience=2, min_delta=1e-05, record_best_weights=True)\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(SelfMdl.parameters(), lr=1e-3)\n",
    "# lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.97)\n",
    "loss_info = SelfMdl.fit(train_dataloader = trainloader, augmenter=Augmenter, epochs=5,\n",
    "                        optimizer=optimizer, loss_func= loss, loss_args= loss_arg,\n",
    "                        lr_scheduler= scheduler, EarlyStopper=earlystop,\n",
    "                        validation_dataloader=valloader,\n",
    "                        verbose=True, device= device, return_loss_info=True\n",
    "                       )\n",
    "print('   SimCLR OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moco\n",
    "print('Testing MoCo v2 (5 epochs, verbose False)...')\n",
    "\n",
    "NNencoder= zoo.EEGNetEncoder(Chans=Chan, kernLength=65)\n",
    "SelfMdl = ssl.MoCo(encoder=NNencoder, projection_head=head_size, bank_size=1024, m=0.9995).to(device=device)\n",
    "\n",
    "# loss (fit method has a default loss based on the SSL algorithm\n",
    "loss=losses.Moco_loss\n",
    "loss_arg={'temperature': 0.5}\n",
    "\n",
    "# earlystopper\n",
    "earlystop = ssl.EarlyStopping(patience=6, min_delta=1e-05, record_best_weights=True)\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(SelfMdl.parameters(), lr=1e-4)\n",
    "# lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "loss_info = SelfMdl.fit(train_dataloader = trainloader, augmenter=Augmenter, epochs=5,\n",
    "                        optimizer=optimizer, loss_func= loss, loss_args= loss_arg,\n",
    "                        lr_scheduler= scheduler, EarlyStopper=earlystop,\n",
    "                        validation_dataloader=valloader,\n",
    "                        verbose=False, device= device, return_loss_info=True\n",
    "                       )\n",
    "print('   MoCo v2 OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moco\n",
    "print('Testing MoCo v3 (5 epochs, verbose False)...')\n",
    "\n",
    "NNencoder= zoo.EEGNetEncoder(Chans=Chan, kernLength=65)\n",
    "SelfMdl = ssl.MoCo(encoder=NNencoder, projection_head=head_size, predictor=predictor_size, m=0.9995).to(device=device)\n",
    "\n",
    "# loss (fit method has a default loss based on the SSL algorithm\n",
    "loss=losses.Moco_loss\n",
    "loss_arg={'temperature': 0.5}\n",
    "\n",
    "# earlystopper\n",
    "earlystop = ssl.EarlyStopping(patience=6, min_delta=1e-05, record_best_weights=True)\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(SelfMdl.parameters(), lr=1e-4)\n",
    "# lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "loss_info = SelfMdl.fit(train_dataloader = trainloader, augmenter=Augmenter, epochs=5,\n",
    "                        optimizer=optimizer, loss_func= loss, loss_args= loss_arg,\n",
    "                        lr_scheduler= scheduler, EarlyStopper=earlystop,\n",
    "                        validation_dataloader=valloader,\n",
    "                        verbose=False, device= device, return_loss_info=True\n",
    "                       )\n",
    "print('   MoCo v3 OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moco\n",
    "print('Testing BYOL (5 epochs, verbose False)...')\n",
    "\n",
    "NNencoder= zoo.EEGNetEncoder(Chans=Chan, kernLength=65)\n",
    "SelfMdl = ssl.BYOL(encoder=NNencoder, projection_head=head_size, predictor=predictor_size, m=0.9995).to(device=device)\n",
    "\n",
    "# loss (fit method has a default loss based on the SSL algorithm\n",
    "loss=losses.BYOL_loss\n",
    "\n",
    "# earlystopper\n",
    "earlystop = ssl.EarlyStopping(patience=6, min_delta=1e-05, record_best_weights=True)\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(SelfMdl.parameters(), lr=1e-4)\n",
    "# lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "loss_info = SelfMdl.fit(train_dataloader = trainloader, augmenter=Augmenter, epochs=5,\n",
    "                        optimizer=optimizer, loss_func= loss,\n",
    "                        lr_scheduler= scheduler, EarlyStopper=earlystop,\n",
    "                        validation_dataloader=valloader,\n",
    "                        verbose=False, device= device, return_loss_info=True\n",
    "                       )\n",
    "print('   BYOL OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moco\n",
    "print('Testing SimSiam (5 epochs, verbose False)...')\n",
    "\n",
    "NNencoder= zoo.EEGNetEncoder(Chans=Chan, kernLength=65)\n",
    "SelfMdl = ssl.SimSiam(encoder=NNencoder, projection_head=head_size, predictor=predictor_size).to(device=device)\n",
    "\n",
    "# loss (fit method has a default loss based on the SSL algorithm\n",
    "loss=losses.SimSiam_loss\n",
    "\n",
    "# earlystopper\n",
    "earlystop = ssl.EarlyStopping(patience=6, min_delta=1e-05, record_best_weights=True)\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(SelfMdl.parameters(), lr=1e-4)\n",
    "# lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "loss_info = SelfMdl.fit(train_dataloader = trainloader, augmenter=Augmenter, epochs=5,\n",
    "                        optimizer=optimizer, loss_func= loss,\n",
    "                        lr_scheduler= scheduler, EarlyStopper=earlystop,\n",
    "                        validation_dataloader=valloader,\n",
    "                        verbose=False, device= device, return_loss_info=True\n",
    "                       )\n",
    "print('   SimSiam OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing VICReg (5 epochs, verbose True)...')\n",
    "\n",
    "NNencoder= zoo.EEGNetEncoder(Chans=Chan, kernLength=65)\n",
    "SelfMdl = ssl.VICReg(encoder=NNencoder, projection_head=head_size).to(device=device)\n",
    "\n",
    "# loss (fit method has a default loss based on the SSL algorithm\n",
    "loss=losses.VICReg_loss\n",
    "\n",
    "# earlystopper\n",
    "earlystop = ssl.EarlyStopping(patience=2, min_delta=1e-05, record_best_weights=True)\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(SelfMdl.parameters(), lr=1e-3)\n",
    "# lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.97)\n",
    "loss_info = SelfMdl.fit(train_dataloader = trainloader, augmenter=Augmenter, epochs=5,\n",
    "                        optimizer=optimizer, loss_func= loss,\n",
    "                        lr_scheduler= scheduler, EarlyStopper=earlystop,\n",
    "                        validation_dataloader=valloader,\n",
    "                        verbose=False, device= device, return_loss_info=True\n",
    "                       )\n",
    "print('   VICReg OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing Barlow_Twins (5 epochs, verbose True)...')\n",
    "\n",
    "NNencoder= zoo.EEGNetEncoder(Chans=Chan, kernLength=65)\n",
    "SelfMdl = ssl.Barlow_Twins(encoder=NNencoder, projection_head=head_size).to(device=device)\n",
    "\n",
    "# loss (fit method has a default loss based on the SSL algorithm\n",
    "loss=losses.Barlow_loss\n",
    "\n",
    "# earlystopper\n",
    "earlystop = ssl.EarlyStopping(patience=2, min_delta=1e-05, record_best_weights=True)\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(SelfMdl.parameters(), lr=1e-3)\n",
    "# lr scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.97)\n",
    "loss_info = SelfMdl.fit(train_dataloader = trainloader, augmenter=Augmenter, epochs=5,\n",
    "                        optimizer=optimizer, loss_func= loss,\n",
    "                        lr_scheduler= scheduler, EarlyStopper=earlystop,\n",
    "                        validation_dataloader=valloader,\n",
    "                        verbose=False, device= device, return_loss_info=True\n",
    "                       )\n",
    "print('   Barlow_Twins OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('testing fine-tuning phase (10 epochs, verbose True)...')\n",
    "\n",
    "# Extract only the samples for fine-tuning\n",
    "filesFT= EEGsplit.loc[EEGsplit['split_set']==2, 'file_name'].values\n",
    "EEGlenFT= EEGlen.loc[EEGlen['file_name'].isin(filesFT)].reset_index().drop(columns=['index'])\n",
    "labels = classes[ EEGsplit[EEGsplit['split_set']==2].index.tolist()]\n",
    "\n",
    "# split the fine-tuning data in train-test-validation\n",
    "EEGsplitFT = dl.GetEEGSplitTable(partition_table=EEGlenFT, test_ratio = 0.2, val_ratio= 0.1, val_ratio_on_all_data=False,\n",
    "                                 stratified=True, labels=labels, split_tolerance=0.001, perseverance=10000)\n",
    "\n",
    "# TRAINING DATALOADER\n",
    "trainsetFT = dl.EEGDataset(EEGlenFT, EEGsplitFT, [freq, window, overlap], 'train', \n",
    "                           supervised=True, label_on_load=True, \n",
    "                           load_function=loadEEG, optional_load_fun_args=[True])\n",
    "trainsamplerFT = dl.EEGsampler(trainsetFT, batchsize, workers)\n",
    "trainloaderFT = DataLoader(dataset = trainsetFT, batch_size= batchsize, sampler=trainsamplerFT, num_workers=workers)\n",
    "\n",
    "# VALIDATION DATALOADER\n",
    "valsetFT = dl.EEGDataset(EEGlenFT, EEGsplitFT, [freq, window, overlap], 'validation', \n",
    "                         supervised=True, label_on_load=True, \n",
    "                         load_function=loadEEG, optional_load_fun_args=[True])\n",
    "valloaderFT = DataLoader(dataset = valsetFT, batch_size= batchsize, num_workers=workers, shuffle=False)\n",
    "\n",
    "#TEST DATALOADER\n",
    "testsetFT = dl.EEGDataset(EEGlenFT, EEGsplitFT, [freq, window, overlap], 'test', \n",
    "                          supervised=True, label_on_load=True, \n",
    "                          load_function=loadEEG, optional_load_fun_args=[True])\n",
    "testloaderFT = DataLoader(dataset = testsetFT, batch_size= batchsize, shuffle=False)\n",
    "\n",
    "FinalMdl = zoo.EEGNet(nb_classes = 2, Chans = Chan, Samples = int(freq*window), kernLength = 65)\n",
    "\n",
    "# Transfer the pretrained backbone and move the final model to the right device\n",
    "SelfMdl.train() \n",
    "SelfMdl.to(device='cpu') \n",
    "FinalMdl.encoder = SelfMdl.get_encoder()\n",
    "FinalMdl.train()\n",
    "FinalMdl.to(device=device)\n",
    "\n",
    "# DEFINE LOSS\n",
    "def loss_fineTuning(yhat, ytrue):\n",
    "    ytrue = ytrue + 0.\n",
    "    yhat = torch.squeeze(yhat)\n",
    "    return F.binary_cross_entropy_with_logits(yhat, ytrue, pos_weight = torch.tensor([2.5]).to(device=device) )\n",
    "\n",
    "# DEFINE EARLYSTOPPER\n",
    "earlystopFT = ssl.EarlyStopping(patience=10, min_delta=1e-03, record_best_weights=True)\n",
    "\n",
    "# DEFINE OPTIMIZER \n",
    "optimizerFT = torch.optim.Adam(FinalMdl.parameters(), lr=1e-3)\n",
    "schedulerFT = torch.optim.lr_scheduler.ExponentialLR(optimizerFT, gamma=0.97)\n",
    "\n",
    "finetuning_loss=ssl.fine_tune(model                 = FinalMdl,\n",
    "                              train_dataloader      = trainloaderFT,\n",
    "                              epochs                = 10,\n",
    "                              optimizer             = optimizerFT,\n",
    "                              loss_func             = loss_fineTuning, \n",
    "                              lr_scheduler          = schedulerFT,\n",
    "                              EarlyStopper          = earlystopFT,\n",
    "                              validation_dataloader = valloaderFT,\n",
    "                              verbose               = True,\n",
    "                              device                = device,\n",
    "                              return_loss_info      = True\n",
    "                             )\n",
    "\n",
    "print('   fine-tuning OK')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
