<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>selfeeg.losses &mdash; SelfEEG 0.0.1 documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            SelfEEG
          </a>
              <div class="version">
                0.0.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"></div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">SelfEEG</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Module code</a></li>
      <li class="breadcrumb-item active">selfeeg.losses</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for selfeeg.losses</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;SimCLR_loss&#39;</span><span class="p">,</span> <span class="s1">&#39;SimSiam_loss&#39;</span><span class="p">,</span> <span class="s1">&#39;Moco_loss&#39;</span><span class="p">,</span> <span class="s1">&#39;BYOL_loss&#39;</span><span class="p">,</span> <span class="s1">&#39;Barlow_loss&#39;</span><span class="p">,</span> <span class="s1">&#39;VICReg_loss&#39;</span><span class="p">]</span>

<div class="viewcode-block" id="SimCLR_loss"><a class="viewcode-back" href="../../selfeeg.html#selfeeg.losses.SimCLR_loss">[docs]</a><span class="k">def</span> <span class="nf">SimCLR_loss</span><span class="p">(</span><span class="n">projections</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> 
                <span class="n">projections_norm</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span>
               <span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    SimCLR compute the normalized temperature-scaled cross entropy loss, which is used in many </span>
<span class="sd">    contrastive learning algorithm. It is basically a simple implementation of the InfoNCE_loss</span>
<span class="sd">    provided in the official simCLR repository using only torch functions.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    projections: torch.Tensor</span>
<span class="sd">        2-D Tensor where projections[0:N/2] are the projections of one batch augmented version</span>
<span class="sd">        and projections[N/2:] are the projections of the other batch augmented version</span>
<span class="sd">    projections_norm: bool, optional</span>
<span class="sd">        whether to normalize the projections or not</span>
<span class="sd">        Default= True</span>
<span class="sd">    temperature: float, optional</span>
<span class="sd">        temperature coefficient of the NTX_ent loss. (See references to check loss formula)</span>
<span class="sd">        Default: 0.15</span>
<span class="sd">    </span>
<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    To check how NTXent work and how it is used to compute the loss read:</span>
<span class="sd">    Chen et al. A Simple Framework for Contrastive Learning of Visual Representations. (2020).</span>
<span class="sd">    https://doi.org/10.48550/arXiv.2002.05709</span>
<span class="sd">    </span>
<span class="sd">    To check the original tensorflow implementation visit the following repository:</span>
<span class="sd">    https://github.com/google-research/simclr (look at the function add_contrastive_loss </span>
<span class="sd">    in objective.py)</span>

<span class="sd">    NOTE:</span>
<span class="sd">    looking at some implementations (e.g. the one in lightlyAI), the returned loss seems to be double. </span>
<span class="sd">    However the function add_contrastive_loss in the original repo return the same value as this </span>
<span class="sd">    implementation, so we preferred to keep it the same.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">projections_norm</span><span class="p">:</span>
        <span class="n">projections</span> <span class="o">=</span>  <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">projections</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#L2 norm along first dimension</span>
    
    
    <span class="n">proj1</span><span class="p">,</span> <span class="n">proj2</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">projections</span> <span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="n">projections</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">N</span><span class="o">=</span><span class="n">proj1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">labels</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">projections</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">masks</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">projections</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">nn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">proj1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">proj1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">temperature</span>
    <span class="n">nn</span> <span class="o">=</span> <span class="n">nn</span> <span class="o">-</span> <span class="p">(</span><span class="n">masks</span> <span class="o">*</span> <span class="mf">1e9</span><span class="p">)</span>
    <span class="n">mm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">proj2</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">proj2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">temperature</span>
    <span class="n">mm</span> <span class="o">=</span> <span class="n">mm</span> <span class="o">-</span> <span class="p">(</span><span class="n">masks</span> <span class="o">*</span> <span class="mf">1e9</span><span class="p">)</span>
    <span class="n">nm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">proj1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">proj2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">temperature</span>
    <span class="n">mn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">proj2</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">proj1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">temperature</span>

    <span class="n">loss_1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">nm</span><span class="p">,</span> <span class="n">nn</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">labels</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
    <span class="n">loss_2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">mn</span><span class="p">,</span> <span class="n">mm</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">labels</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_1</span> <span class="o">+</span> <span class="n">loss_2</span>

    <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="SimSiam_loss"><a class="viewcode-back" href="../../selfeeg.html#selfeeg.losses.SimSiam_loss">[docs]</a><span class="k">def</span> <span class="nf">SimSiam_loss</span><span class="p">(</span><span class="n">p1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> 
                 <span class="n">z1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> 
                 <span class="n">p2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> 
                 <span class="n">z2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">projections_norm</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simple implementation of the SimSiam loss with the possibility to not normalize tensors.</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    p1: torch.Tensor</span>
<span class="sd">        2-D Tensor with one augmented batch predictor output</span>
<span class="sd">    z1: torch.Tensor</span>
<span class="sd">        2-D Tensor with one augmented batch projection output</span>
<span class="sd">    p2: torch.Tensor</span>
<span class="sd">        same as p1 but with the other augmented batch</span>
<span class="sd">    z2: torch.Tensor</span>
<span class="sd">        same as z1 with the other augmented batch</span>
<span class="sd">    projections_norm: bool, optional</span>
<span class="sd">        whether to normalize the projections or not</span>
<span class="sd">        Default= True</span>

<span class="sd">    Original github repo:</span>
<span class="sd">    https://github.com/facebookresearch/simsiam</span>

<span class="sd">    Original paper:</span>
<span class="sd">    Chen &amp; He. Exploring Simple Siamese Representation Learning. </span>
<span class="sd">    https://arxiv.org/abs/2011.10566</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">projections_norm</span><span class="p">:</span>
        <span class="n">p1</span><span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">z1</span><span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">z1</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">p2</span><span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">p2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">z2</span><span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">z2</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">D1</span><span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">p1</span><span class="o">*</span><span class="n">z2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">D2</span><span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">p2</span><span class="o">*</span><span class="n">z1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">D1</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">D2</span>
    <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="Moco_loss"><a class="viewcode-back" href="../../selfeeg.html#selfeeg.losses.Moco_loss">[docs]</a><span class="k">def</span> <span class="nf">Moco_loss</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> 
              <span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> 
              <span class="n">queue</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">projections_norm</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.15</span><span class="p">):</span>
<span class="w">    </span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simple implementation of Moco&#39;s loss. It&#39;s the InfoNCE loss with dot product as similarity and</span>
<span class="sd">    memory bank as negative samples. If no queue related to the memory bank is given, Moco v3 loss </span>
<span class="sd">    calculation are performed</span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    q: torch.Tensor</span>
<span class="sd">        2-D (NxC) Tensor with the queries, i.e. one augmented batch predictor or projection_head output.</span>
<span class="sd">        N = batch size, C = number of features </span>
<span class="sd">    k: torch.Tensor</span>
<span class="sd">        2-D (NxC) Tensor with the keys, i.e. one augmented batch projection_head output which will be </span>
<span class="sd">        added to the memory bank. </span>
<span class="sd">        N = batch size </span>
<span class="sd">        C = number of features </span>
<span class="sd">    queue:  torch.Tensor</span>
<span class="sd">        2-D (CxK) Tensor with the memory bank, i.e. a collection of previous augmented batch </span>
<span class="sd">        projection_head outputs which acts as negative samples. </span>
<span class="sd">        C = number of features </span>
<span class="sd">        K = memory bank size</span>
<span class="sd">    projections_norm: bool, optional</span>
<span class="sd">        whether to normalize the projections or not.</span>
<span class="sd">        Default= True</span>
<span class="sd">    temperature: float, optional</span>
<span class="sd">        temperature coefficient of the NTX_ent loss. (See references to check loss formula)</span>
<span class="sd">        Default: 0.15</span>
<span class="sd">    </span>
<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    To check how NTXent work and how it is used to compute the loss read:</span>
<span class="sd">    Chen et al. A Simple Framework for Contrastive Learning of Visual Representations. (2020).</span>
<span class="sd">    https://doi.org/10.48550/arXiv.2002.05709</span>
<span class="sd">    </span>
<span class="sd">    To check the original tensorflow implementation visit the following repository:</span>
<span class="sd">    https://github.com/google-research/simclr (look at the function add_contrastive_loss </span>
<span class="sd">    in objective.py)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1"># normalize</span>
    <span class="k">if</span> <span class="n">projections_norm</span><span class="p">:</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># if no queue is given, run MoCo v3 loss </span>
    <span class="c1"># (note that MoCo v3 is MoCo_loss(q1,k2) + MoCo_loss(q2,k1) </span>
    <span class="k">if</span> <span class="n">queue</span><span class="o">==</span><span class="kc">None</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;nc,mc-&gt;nm&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span> <span class="o">/</span> <span class="n">temperature</span>
        <span class="n">N</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># batch size per GPU</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">temperature</span><span class="p">)</span>
    
    <span class="c1"># positive logits: Nx1</span>
    <span class="n">l_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">C</span><span class="p">),</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># negative logits: NxK</span>
    <span class="n">l_neg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">queue</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
    <span class="c1"># logits: Nx(1+K)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">l_pos</span><span class="p">,</span> <span class="n">l_neg</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># apply temperature</span>
    <span class="n">logits</span> <span class="o">/=</span> <span class="n">temperature</span>
    <span class="c1"># labels: positive key indicators</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">logits</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="BYOL_loss"><a class="viewcode-back" href="../../selfeeg.html#selfeeg.losses.BYOL_loss">[docs]</a><span class="k">def</span> <span class="nf">BYOL_loss</span><span class="p">(</span><span class="n">p1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> 
              <span class="n">z1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> 
              <span class="n">p2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> 
              <span class="n">z2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
              <span class="n">projections_norm</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simple pytorch implementation of the BYOL loss function. </span>
<span class="sd">    It&#39;s pretty similar to SimSiam loss</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">if</span> <span class="n">projections_norm</span><span class="p">:</span>
        <span class="n">p1</span><span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">z1</span><span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">z1</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">p2</span><span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">p2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">z2</span><span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">z2</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">loss1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">p1</span> <span class="o">*</span> <span class="n">z2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">loss2</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">p2</span> <span class="o">*</span> <span class="n">z1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss1</span> <span class="o">+</span> <span class="n">loss2</span>
    <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span></div>

<div class="viewcode-block" id="Barlow_loss"><a class="viewcode-back" href="../../selfeeg.html#selfeeg.losses.Barlow_loss">[docs]</a><span class="k">def</span> <span class="nf">Barlow_loss</span><span class="p">(</span><span class="n">z1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> 
                <span class="n">z2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">lambda_coeff</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">5e-3</span><span class="p">,</span>
               <span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    Pytorch implementation of the Baarlow Twins loss function. </span>

<span class="sd">    Arguments</span>
<span class="sd">    ---------</span>
<span class="sd">    z1: torch.tensor</span>
<span class="sd">        2-D tensor with projections of one augmented version of the batch</span>
<span class="sd">    z2: torch.tensor</span>
<span class="sd">        2-D projections of the other augmented version of the batch. Can be none if z1 and z2 are cat </span>
<span class="sd">        together. In this case internal split is done</span>
<span class="sd">    lambda_coeff: float, optional</span>
<span class="sd">        off diagonal scaling factor described in the paper</span>
<span class="sd">        Default: 5e-3</span>
<span class="sd">        </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">z2</span><span class="o">==</span><span class="kc">None</span><span class="p">:</span>
        <span class="n">z1</span><span class="p">,</span> <span class="n">z2</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">z1</span> <span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="n">z1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span>
    
    <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">z1</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">z1_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">z1</span><span class="o">-</span><span class="n">z1</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">/</span><span class="n">z1</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">z2_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">z2</span><span class="o">-</span><span class="n">z2</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">/</span><span class="n">z2</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">c_mat</span> <span class="o">=</span> <span class="p">(</span><span class="n">z1_norm</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">z2_norm</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span>
    <span class="c1">#c_mat_diff = (c_mat - torch.eye(D, device=c_mat.device)).pow(2)</span>
    <span class="c1">#lambda_mat = torch.full( (D,D), lambda_coeff, device=z1.device)</span>
    <span class="c1">#lambda_mat[range(D), range(D)]=1</span>
    <span class="c1">#loss = (c_mat_diff*lambda_mat).sum()</span>
    
    <span class="n">c_mat2</span> <span class="o">=</span> <span class="n">c_mat</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">D</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">c_mat</span><span class="p">)</span> <span class="o">+</span> <span class="n">lambda_coeff</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">c_mat</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">lambda_coeff</span><span class="p">)</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">c_mat</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span></div>



<div class="viewcode-block" id="VICReg_loss"><a class="viewcode-back" href="../../selfeeg.html#selfeeg.losses.VICReg_loss">[docs]</a><span class="k">def</span> <span class="nf">VICReg_loss</span><span class="p">(</span><span class="n">z1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> 
                <span class="n">z2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">Lambda</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
                <span class="n">Mu</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
                <span class="n">Nu</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-4</span>
               <span class="p">):</span>
    <span class="k">if</span> <span class="n">z2</span><span class="o">==</span><span class="kc">None</span><span class="p">:</span>
        <span class="n">z1</span><span class="p">,</span> <span class="n">z2</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">z1</span> <span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="n">z1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span>

    <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">z1</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">z1</span> <span class="o">=</span> <span class="n">z1</span> <span class="o">-</span> <span class="n">z1</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">z2</span> <span class="o">=</span> <span class="n">z2</span> <span class="o">-</span> <span class="n">z2</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># invariance loss</span>
    <span class="n">sim_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="n">z2</span><span class="p">)</span>

    <span class="c1"># variance loss</span>
    <span class="n">std_z1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">z1</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="n">std_z2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">z2</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="n">std_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">std_z1</span><span class="p">))</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">std_z2</span><span class="p">)))</span><span class="o">/</span><span class="mi">2</span>
    
    <span class="c1"># covariance loss</span>
    <span class="n">cov_z1</span> <span class="o">=</span> <span class="p">(</span><span class="n">z1</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">z1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">cov_z1</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">D</span><span class="p">),</span><span class="nb">range</span><span class="p">(</span><span class="n">D</span><span class="p">)]</span><span class="o">=</span><span class="mf">0.</span>
    <span class="n">cov_z2</span> <span class="o">=</span> <span class="p">(</span><span class="n">z2</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">z2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">cov_z2</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">D</span><span class="p">),</span><span class="nb">range</span><span class="p">(</span><span class="n">D</span><span class="p">)]</span><span class="o">=</span><span class="mf">0.</span>
    <span class="n">cov_loss</span> <span class="o">=</span> <span class="n">cov_z1</span><span class="o">.</span><span class="n">pow_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">D</span> <span class="o">+</span> <span class="n">cov_z2</span><span class="o">.</span><span class="n">pow_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">D</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">Lambda</span> <span class="o">*</span> <span class="n">sim_loss</span> <span class="o">+</span> <span class="n">Mu</span> <span class="o">*</span> <span class="n">std_loss</span> <span class="o">+</span> <span class="n">Nu</span> <span class="o">*</span> <span class="n">cov_loss</span>
    <span class="k">return</span> <span class="n">loss</span></div>


</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, MedMax Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>